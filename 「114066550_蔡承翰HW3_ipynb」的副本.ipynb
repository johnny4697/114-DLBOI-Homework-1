{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnny4697/114-DLBOI-Homework-1/blob/main/%E3%80%8C114066550_%E8%94%A1%E6%89%BF%E7%BF%B0HW3_ipynb%E3%80%8D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Library"
      ],
      "metadata": {
        "id": "8dBIMBMpa8LZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdshpulRajCG"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optims\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms.functional import InterpolationMode"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cpu')\n",
        "gpu_count = torch.cuda.device_count()\n",
        "\n",
        "print(\"Using\", gpu_count, \"GPUs\")\n",
        "print(\"CUDA is available:\", torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "nwp7lRWlbX_s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90a670f4-e889-429a-8f86-6fc8b3664fa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 0 GPUs\n",
            "CUDA is available: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train loop\n",
        "def train_one_epoch(model, device, criterion, optimizer, train_data_loader):\n",
        "    epoch_loss = []\n",
        "    epoch_acc = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate (train_data_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device).float()\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(images)\n",
        "        loss = criterion(preds, labels.unsqueeze(1))\n",
        "\n",
        "        # Calculating Loss\n",
        "        epoch_loss.append(loss.item())\n",
        "\n",
        "        # Calculating Metrics\n",
        "        predicts = (preds > 0.5).float()\n",
        "        predicts = predicts.view(-1)\n",
        "        predicts = predicts.detach().cpu().numpy()\n",
        "        labels = labels.detach().cpu().numpy()\n",
        "        acc = accuracy_score(labels, predicts)\n",
        "\n",
        "        epoch_acc.append(acc)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Overall Epoch Results\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "\n",
        "    epoch_loss = np.mean(epoch_loss)\n",
        "    epoch_acc = np.mean(epoch_acc) * 100\n",
        "\n",
        "    return epoch_loss, epoch_acc, total_time\n",
        "\n",
        "# validation loop\n",
        "def val_one_epoch(model, device, criterion, val_data_loader, best_acc, save):\n",
        "    epoch_loss = []\n",
        "    epoch_acc = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_data_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device).float()\n",
        "            preds = model(images)\n",
        "\n",
        "            # Calculating Loss\n",
        "            loss = criterion(preds, labels.unsqueeze(1))\n",
        "            epoch_loss.append(loss.item())\n",
        "\n",
        "            # Calculating Metrics\n",
        "            predicts = (preds > 0.5).float()\n",
        "            predicts = predicts.view(-1)\n",
        "            predicts = predicts.detach().cpu().numpy()\n",
        "            labels = labels.detach().cpu().numpy()\n",
        "            acc = accuracy_score(labels, predicts)\n",
        "            epoch_acc.append(acc)\n",
        "\n",
        "    # Overall Epoch Results\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "\n",
        "    epoch_loss = np.mean(epoch_loss)\n",
        "    epoch_acc = np.mean(epoch_acc) * 100\n",
        "\n",
        "    # Saving best model\n",
        "    if epoch_acc > best_acc:\n",
        "        best_acc = epoch_acc\n",
        "        torch.save(model.state_dict(), f\"{save}.pth\")\n",
        "\n",
        "    return epoch_loss, epoch_acc, total_time, best_acc\n",
        "\n",
        "# evaluate loop (test loop)\n",
        "def evaluate(model, device, model_path, test_loader):\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "        print(\"Model weights loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(\"Warning: Failed to load model weights. Using randomly initialized weights instead.\")\n",
        "        print(e)\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    test_loss = []\n",
        "    test_acc = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device).float()\n",
        "\n",
        "            # Forward pass\n",
        "            preds = model(images)\n",
        "\n",
        "            # Calculating loss\n",
        "            loss = criterion(preds, labels.unsqueeze(1))\n",
        "            test_loss.append(loss.item())\n",
        "\n",
        "            # Calculating accuracy\n",
        "            predicts = (preds > 0.5).float()\n",
        "            predicts = predicts.view(-1)\n",
        "            predicts = predicts.detach().cpu().numpy()\n",
        "            labels = labels.detach().cpu().numpy()\n",
        "            acc = accuracy_score(labels, predicts)\n",
        "            test_acc.append(acc)\n",
        "\n",
        "    # Overall test results\n",
        "    avg_test_loss = np.mean(test_loss)\n",
        "    avg_test_acc = np.mean(test_acc) * 100\n",
        "\n",
        "    print(f\"Test Accuracy: {avg_test_acc:.2f}%\")\n",
        "    print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "\n",
        "    return avg_test_loss, avg_test_acc"
      ],
      "metadata": {
        "id": "O-QHGGMMbBur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "V5Mz1-uybSmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "rWULx9svbXJD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7fd4fd7-77bc-4986-d5d8-fc941315460a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define folder path for each set\n",
        "train_path = '/content/drive/MyDrive/DLBOI.hw2/train'\n",
        "test_path = '/content/drive/MyDrive/DLBOI.hw2/test'\n",
        "val_path = '/content/drive/MyDrive/DLBOI.hw2/val'\n",
        "\n",
        "# define transformation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256), interpolation=InterpolationMode.BICUBIC),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "common_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256), interpolation=InterpolationMode.BICUBIC),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# create datasets\n",
        "train_dataset = datasets.ImageFolder(train_path, transform=train_transform)\n",
        "test_dataset = datasets.ImageFolder(test_path, transform=common_transform)\n",
        "val_dataset = datasets.ImageFolder(val_path, transform=common_transform)\n",
        "\n",
        "# create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "DpL62qzsbkcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "QxPrGZFPmvoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(256 * 256 * 1, 64),\n",
        "    nn.BatchNorm1d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "\n",
        "    nn.Linear(64, 64),\n",
        "    nn.BatchNorm1d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "\n",
        "    nn.Linear(64, 64),\n",
        "    nn.BatchNorm1d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "\n",
        "    nn.Linear(64, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "model.to('cpu')\n",
        "print(model)"
      ],
      "metadata": {
        "id": "ml-6AVUimusM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bafafeb-a7c0-4ab2-cb16-fb55cf194ca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Flatten(start_dim=1, end_dim=-1)\n",
            "  (1): Linear(in_features=65536, out_features=64, bias=True)\n",
            "  (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (3): ReLU()\n",
            "  (4): Dropout(p=0.5, inplace=False)\n",
            "  (5): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (7): ReLU()\n",
            "  (8): Dropout(p=0.5, inplace=False)\n",
            "  (9): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (11): ReLU()\n",
            "  (12): Dropout(p=0.5, inplace=False)\n",
            "  (13): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (14): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loop"
      ],
      "metadata": {
        "id": "SdTAY4X1CXUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameter\n",
        "lr = 0.0001\n",
        "weight_decay = 0.001\n",
        "lr_scheduler = optims.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=5, mode='max')\n",
        "epochs = 10\n",
        "optimizer = optims.Adam(model.parameters(), lr=lr, weight_decay = weight_decay)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# save checkpoint\n",
        "save = 'model'"
      ],
      "metadata": {
        "id": "w_EzCFAjCWp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "best_acc = 0.0\n",
        "output_list = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss, train_acc, train_time = train_one_epoch(model, device, criterion, optimizer, train_loader)\n",
        "    val_loss, val_acc, val_time, best_acc = val_one_epoch(model, device, criterion, val_loader, best_acc, save)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    total_time = train_time + val_time\n",
        "    output_str = f\"Epoch {epoch+1}/{epochs} - loss: {train_loss:.4f} - train_acc: {train_acc:.2f}% - val_loss: {val_loss:.4f} - val_acc: {val_acc:.2f}% - time: {total_time:.2f}s\"\n",
        "    output_list.append(output_str)\n",
        "    print(output_str)\n",
        "    lr_scheduler.step(val_acc)"
      ],
      "metadata": {
        "id": "oxdqJu8wCae3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fcfcacc-15fa-425b-b0c8-b0c3fb4904f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - loss: 0.6536 - train_acc: 60.64% - val_loss: 0.6179 - val_acc: 68.75% - time: 79.75s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph"
      ],
      "metadata": {
        "id": "UGz-VUPmCgf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# loss graph\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.title('Loss Curve')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# accuracy graph\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracies, label='Train Accuracy')\n",
        "plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "plt.title('Accuracy Curve')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "\n",
        "# show\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "an1QmoF0CjAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate"
      ],
      "metadata": {
        "id": "W5W5mX4KH0eg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = 'model.pth'\n",
        "avg_test_loss, avg_test_acc = evaluate(model, device, model_path, test_loader)"
      ],
      "metadata": {
        "id": "50oemy4aH2nW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}